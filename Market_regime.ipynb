{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import argrelextrema\n",
    "import matplotlib.pyplot as plt\n",
    "from untrade.client import Client\n",
    "import ta\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from arch import arch_model  # For GARCH model\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Unnamed: 0      open      high       low     close  \\\n",
      "datetime                                                                  \n",
      "2019-09-08 17:30:00           0  10000.00  10000.00  10000.00  10000.00   \n",
      "2019-09-08 18:00:00           1  10000.00  10000.00  10000.00  10000.00   \n",
      "2019-09-08 18:30:00           2  10000.00  10000.00  10000.00  10000.00   \n",
      "2019-09-08 19:00:00           3  10344.77  10357.53  10342.90  10354.62   \n",
      "2019-09-08 19:30:00           4  10354.62  10357.35  10337.43  10340.12   \n",
      "\n",
      "                      volume  \n",
      "datetime                      \n",
      "2019-09-08 17:30:00    0.002  \n",
      "2019-09-08 18:00:00    0.000  \n",
      "2019-09-08 18:30:00    0.000  \n",
      "2019-09-08 19:00:00  136.177  \n",
      "2019-09-08 19:30:00  335.482  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\n",
    "    '/Users/tejasmacipad/Desktop/Final_inter_IIT_submission/BTC/BTC_2019_2023_30m.csv',\n",
    "    parse_dates=['datetime'],  # Parse the 'datetime' column as datetime\n",
    "    index_col='datetime'       # Use 'datetime' as the index\n",
    ")\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Tech indicators for calculation of Market Regime:\n",
    "\n",
    "1) Bearish Market\n",
    "2) Bullish Market\n",
    "3) Sideways Market\n",
    "4) High Volatilite Market\n",
    "5) Trend Reversal Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_data = data[[\"close\"]].copy()\n",
    "\n",
    "def feature_engineering(data):\n",
    "    mod_data = data[[\"close\"]].copy()\n",
    "    \n",
    "    mod_data['return'] = data['close'].pct_change()  \n",
    "    mod_data['log_return'] = np.log(data['close'] / data['close'].shift(1)) \n",
    "    mod_data['volatility'] = mod_data['return'].rolling(window=20).std()  \n",
    "    mod_data['rsi'] = ta.momentum.RSIIndicator(data['close']).rsi()\n",
    "    mod_data['macd_diff'] = ta.trend.MACD(data['close']).macd_diff()\n",
    "    bollinger = ta.volatility.BollingerBands(data['close'])\n",
    "    mod_data['bollinger_hband'] = bollinger.bollinger_hband()\n",
    "    mod_data['bollinger_lband'] = bollinger.bollinger_lband()\n",
    "\n",
    "    mod_data['scaled_return'] = mod_data['return'] * 100  \n",
    "    \n",
    "    garch_model = arch_model(mod_data['scaled_return'].dropna(), vol='Garch', p=1, q=1, rescale=False)\n",
    "    garch_fit = garch_model.fit(disp=\"off\")\n",
    "    mod_data['garch_volatility'] = np.nan\n",
    "    mod_data.iloc[1:, mod_data.columns.get_loc('garch_volatility')] = garch_fit.conditional_volatility.values / 100\n",
    "\n",
    "    mod_data.dropna(inplace=True)\n",
    "\n",
    "    return mod_data\n",
    "\n",
    "historical_mod_data = feature_engineering(historical_data)\n",
    "features = historical_mod_data[['volatility', 'rsi', 'macd_diff', 'bollinger_hband', 'bollinger_lband', 'garch_volatility']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "optimal_clusters = None\n",
    "best_score = -1\n",
    "for n_clusters in range(2, 6):  \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(features_scaled)\n",
    "    score = silhouette_score(features_scaled, labels)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        optimal_clusters = n_clusters\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "historical_mod_data['regime'] = kmeans.fit_predict(features_scaled)\n",
    "\n",
    "regime_mapping = {0: 'Bullish', 1: 'Bearish', 2: 'Sideways'}\n",
    "historical_mod_data['regime'] = historical_mod_data['regime'].map(regime_mapping)\n",
    "\n",
    "vol_mean = historical_mod_data['garch_volatility'].mean()\n",
    "vol_std = historical_mod_data['garch_volatility'].std()\n",
    "high_vol_threshold = vol_mean + vol_std\n",
    "low_vol_threshold = vol_mean - vol_std\n",
    "historical_mod_data['volatility_regime'] = np.where(historical_mod_data['garch_volatility'] > high_vol_threshold, 'High Volatility',\n",
    "                                                   np.where(historical_mod_data['garch_volatility'] < low_vol_threshold, 'Low Volatility',\n",
    "                                                            'Moderate Volatility'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refined Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "updating_data = pd.DataFrame(columns=['close', 'return', 'log_return', 'volatility', 'rsi', 'macd_diff', \n",
    "                                    'bollinger_hband', 'bollinger_lband', 'scaled_return', 'garch_volatility', 'regime'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "kmeans = None\n",
    "\n",
    "def feature_engineering(data):\n",
    "    mod_data = data[[\"close\"]].copy()\n",
    "    mod_data['return'] = data['close'].pct_change()  \n",
    "    mod_data['log_return'] = np.log(data['close'] / data['close'].shift(1))  \n",
    "    mod_data['volatility'] = mod_data['return'].rolling(window=20).std()  \n",
    "    mod_data['rsi'] = ta.momentum.RSIIndicator(data['close']).rsi()\n",
    "    mod_data['macd_diff'] = ta.trend.MACD(data['close']).macd_diff()\n",
    "    bollinger = ta.volatility.BollingerBands(data['close'])\n",
    "    mod_data['bollinger_hband'] = bollinger.bollinger_hband()\n",
    "    mod_data['bollinger_lband'] = bollinger.bollinger_lband()\n",
    "    mod_data['scaled_return'] = mod_data['return'] * 100  \n",
    "\n",
    "    garch_model = arch_model(mod_data['scaled_return'].dropna(), vol='Garch', p=1, q=1, rescale=False)\n",
    "    garch_fit = garch_model.fit(disp=\"off\")\n",
    "    mod_data['garch_volatility'] = np.nan\n",
    "    mod_data.iloc[1:, mod_data.columns.get_loc('garch_volatility')] = garch_fit.conditional_volatility.values / 100\n",
    "    mod_data.dropna(inplace=True)\n",
    "    return mod_data\n",
    "\n",
    "def initialize_models(historical_data):\n",
    "    global updating_data, kmeans, scaler\n",
    "    \n",
    "    print(\"Starting feature engineering...\")\n",
    "    initial_features = feature_engineering(historical_data)\n",
    "    \n",
    "    features = initial_features[['volatility', 'rsi', 'macd_diff', \n",
    "                               'bollinger_hband', 'bollinger_lband', 'garch_volatility']]\n",
    "    \n",
    "    print(\"Finding optimal number of clusters...\")\n",
    "    optimal_clusters = None\n",
    "    best_score = -1\n",
    "    \n",
    "    for n_clusters in range(2, 6):\n",
    "        kmeans_temp = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        features_scaled = scaler.fit_transform(features)\n",
    "        labels = kmeans_temp.fit_predict(features_scaled)\n",
    "        score = silhouette_score(features_scaled, labels)\n",
    "        print(f\"Clusters: {n_clusters}, Silhouette Score: {score:.3f}\")\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            optimal_clusters = n_clusters\n",
    "    \n",
    "    print(f\"Optimal number of clusters: {optimal_clusters}\")\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    kmeans.fit(features_scaled)\n",
    "    \n",
    "    vol_mean = initial_features['garch_volatility'].mean()\n",
    "    vol_std = initial_features['garch_volatility'].std()\n",
    "    global high_vol_threshold, low_vol_threshold\n",
    "    high_vol_threshold = vol_mean + vol_std\n",
    "    low_vol_threshold = vol_mean - vol_std\n",
    "\n",
    "\n",
    "def predict_market_condition(new_data):\n",
    "    global updating_data, kmeans, high_vol_threshold, low_vol_threshold\n",
    "    \n",
    "    new_mod_data = feature_engineering(new_data)\n",
    "    updating_data = pd.concat([updating_data, new_mod_data])\n",
    "    \n",
    "    if len(updating_data) < 20:\n",
    "        return \"Neutral\", \"Moderate Volatility\"\n",
    "    \n",
    "    features = updating_data[['volatility', 'rsi', 'macd_diff', \n",
    "                            'bollinger_hband', 'bollinger_lband', 'garch_volatility']]\n",
    "    features_scaled = scaler.transform(features)\n",
    "    \n",
    "    predicted_regime = kmeans.predict(features_scaled[-1].reshape(1, -1))[0]\n",
    "    regime_mapping = {0: 'Bullish', 1: 'Bearish', 2: 'Sideways'}\n",
    "    market_condition = regime_mapping.get(predicted_regime, 'Neutral')\n",
    "    \n",
    "    latest_vol = updating_data['garch_volatility'].iloc[-1]\n",
    "    if latest_vol > high_vol_threshold:\n",
    "        vol_regime = 'High Volatility'\n",
    "    elif latest_vol < low_vol_threshold:\n",
    "        vol_regime = 'Low Volatility'\n",
    "    else:\n",
    "        vol_regime = 'Moderate Volatility'\n",
    "    \n",
    "    return market_condition, vol_regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature engineering...\n",
      "Finding optimal number of clusters...\n",
      "Clusters: 2, Silhouette Score: 0.336\n",
      "Clusters: 3, Silhouette Score: 0.344\n",
      "Clusters: 4, Silhouette Score: 0.337\n",
      "Clusters: 5, Silhouette Score: 0.236\n",
      "Optimal number of clusters: 3\n"
     ]
    }
   ],
   "source": [
    "initialize_models(historical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
